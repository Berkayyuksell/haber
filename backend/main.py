from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import torch
from transformers import BertTokenizer, BertForSequenceClassification # BURADA DEÄÄ°ÅÄ°KLÄ°K YAPILDI!
import pickle
import numpy as np
from typing import List, Optional # Optional eklendi

app = FastAPI()


app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # TÃ¼m origin'lere izin ver
    allow_credentials=True,
    allow_methods=["*"], # TÃ¼m HTTP metotlarÄ±na (GET, POST vb.) izin ver
    allow_headers=["*"], # TÃ¼m baÅŸlÄ±klara izin ver
)

model_dir = "../model" 
label_encoder_path = "../label_encoder.pkl" 

# Modeli ve diÄŸer bileÅŸenleri baÅŸlatmak iÃ§in boÅŸ deÄŸiÅŸkenler
tokenizer: Optional[BertTokenizer] = None
model: Optional[BertForSequenceClassification] = None
label_encoder: Optional[pickle._Unpickler] = None
device: torch.device = torch.device("cpu") # 

@app.on_event("startup")
async def load_resources():
    
    global tokenizer, model, label_encoder, device

    try:
        # CihazÄ± belirle
        if torch.cuda.is_available():
            device = torch.device("cuda")
        elif torch.backends.mps.is_available():
            device = torch.device("mps")
        else:
            device = torch.device("cpu")
        print(f"ğŸš€ KullanÄ±lacak cihaz: {device}")

        
        tokenizer = BertTokenizer.from_pretrained(model_dir)
        model = BertForSequenceClassification.from_pretrained(model_dir)
        model.to(device)
        model.eval() 

        # Label encoder'Ä± yÃ¼kle
        with open(label_encoder_path, "rb") as f:
            label_encoder = pickle.load(f)

        print(f"âœ… Model, tokenizer ve label encoder baÅŸarÄ±yla yÃ¼klendi. Cihaz: {device}")

    except Exception as e:
        print(f"âŒ Model veya diÄŸer kaynaklar yÃ¼klenirken hata oluÅŸtu: {e}")
        print(f"LÃ¼tfen '{model_dir}' klasÃ¶rÃ¼nÃ¼n ve '{label_encoder_path}' dosyasÄ±nÄ±n doÄŸru yolda olduÄŸundan ve eÄŸitimden sonra oluÅŸturulduÄŸundan emin olun.")
    


class NewsInput(BaseModel):
    text: str

class PredictionResponse(BaseModel):
    category: str
    confidence: float 

# --- Tahmin Fonksiyonu ---
def predict_category(text: str) -> PredictionResponse: 
    if model is None or tokenizer is None or label_encoder is None:
        raise HTTPException(status_code=500, detail="Model henÃ¼z yÃ¼klenmedi veya yÃ¼klenirken hata oluÅŸtu.")
    max_len_for_inference = 128 
    encoding = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=max_len_for_inference,
        return_token_type_ids=False,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )
    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)
    # Model Ã§Ä±ktÄ±sÄ±nÄ± al
    with torch.no_grad(): 
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
    # Logit'leri al ve olasÄ±lÄ±klara dÃ¶nÃ¼ÅŸtÃ¼r (softmax kullanarak)
    # BertForSequenceClassification doÄŸrudan logit'leri dÃ¶ndÃ¼rÃ¼r.
    logits = outputs.logits
    probabilities = torch.nn.functional.softmax(logits, dim=1)
    # En yÃ¼ksek olasÄ±lÄ±ÄŸa sahip sÄ±nÄ±fÄ± ve gÃ¼ven skorunu bul
    confidence, predicted_idx = torch.max(probabilities, 1)
    # SayÄ±sal etiketi kategori adÄ±na geri dÃ¶nÃ¼ÅŸtÃ¼r
    category = label_encoder.inverse_transform([predicted_idx.item()])[0]
    # SonuÃ§larÄ± PredictionResponse modeline uygun ÅŸekilde dÃ¶ndÃ¼r
    return PredictionResponse(category=category, confidence=confidence.item())

# --- API Endpoints ---

@app.get("/")
async def root():
    """API'nin durumunu kontrol etmek iÃ§in basit bir endpoint."""
    return {"message": "News Category Classification API is running. Send POST requests to /predict or /predict_multiple."}

@app.post("/predict", response_model=PredictionResponse)
async def predict_news_category(news: NewsInput):
    """Tek bir haber baÅŸlÄ±ÄŸÄ±nÄ±n kategorisini tahmin eder."""
    try:
        result = predict_category(news.text)
        return result
    except HTTPException as e: # HTTPException'Ä± yakala ve yeniden fÄ±rlat
        raise e
    except Exception as e: # DiÄŸer genel hatalarÄ± yakala
        raise HTTPException(status_code=500, detail=f"Tahmin sÄ±rasÄ±nda beklenmeyen bir hata oluÅŸtu: {str(e)}")

@app.post("/predict_multiple", response_model=List[PredictionResponse])
async def predict_multiple_news_categories(news_items: List[NewsInput]):
    """Birden fazla haber baÅŸlÄ±ÄŸÄ±nÄ±n kategorilerini tahmin eder."""
    results = []
    try:
        for news_item in news_items:
            prediction = predict_category(news_item.text)
            results.append(prediction)
        return results
    except HTTPException as e: # HTTPException'Ä± yakala ve yeniden fÄ±rlat
        raise e
    except Exception as e: # DiÄŸer genel hatalarÄ± yakala
        raise HTTPException(status_code=500, detail=f"Ã‡oklu tahmin sÄ±rasÄ±nda beklenmeyen bir hata oluÅŸtu: {str(e)}")